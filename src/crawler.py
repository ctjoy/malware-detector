# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import subprocess
import requests
import argparse

__version__ = '1.0'

def crawler():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='''
         A crawler for the malware detector, it download benign executable files from Download.com.
         Input: number pages you want to crawl from its software page rank and the save path
         Output: executable files
    ''')
    parser.add_argument('-n', metavar='NUMBER_PAGES', help='Number pages you want to crawl', required=True)
    parser.add_argument('-p', metavar='SAVE_PATH', help="Save path for executable files", required=True)
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)

    args = parser.parse_args()
    number_pages = int(args.n)
    save_path = args.p

    DOMAIN = 'http://download.cnet.com/s/software/windows/'

    startPage = 14
    urls = ['{}?page={}'.format(DOMAIN, i) for i in range(startPage, startPage + number_pages)]
    exceptList = list();
    pageCounter = startPage
    for url in urls:
        print('\033[1;31mDownload page: {0} ...\033[0m'.format(pageCounter))
        pageCounter += 1
        response = requests.get(url=url)
        soup = BeautifulSoup(response.text, "lxml")
        item_links = soup.find(id="search-results").find_all('a', limit=10)
        for item in item_links:
            try:
                download_link = parse_page(item['href'])
                download(download_link, save_path)
            except:
                exceptList.append(item['href'])
                continue

    print("the following links(total:{0}) are invalid ...".format(len(exceptList)))
    for ex_link in exceptList:
        print(ex_link)

def parse_page(link):
    response = requests.get(url=link)
    soup = BeautifulSoup(response.text, "lxml")
    return soup.find(id="product-upper-container").find('div').find('div').find('div')['data-dl-url']

def download(download_link, save_path):
    subprocess.call(["wget", "-P", save_path, download_link])

if __name__ == '__main__':
    crawler()
