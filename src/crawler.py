#!/usr/bin/python3
# -*- coding: utf-8 -*-
import os
import sys
import argparse
import threading

import requests
from bs4 import BeautifulSoup

from config.proj import DOMAIN, SAVE_PATH
from src.ultis import log, logErr, logWarn

__version__ = '1.3'

def crawler(argv):
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='''
         A crawler for the malware detector, it download benign executable files from Download.com.
         Input: number pages you want to crawl from its software page rank and the save path
         Output: executable files
    ''')
    parser.add_argument('-s', metavar='START_PAGE', help='Number pages you want to START crawl', type=int,  default=1)
    parser.add_argument('-n', metavar='NUMBER_PAGES', help='Number pages you want to crawl', type=int, default=1)
    parser.add_argument('-p', metavar='SAVE_PATH', help="Save path for executable files", default=SAVE_PATH)
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)

    args = parser.parse_args(argv[1:])
    start_page =args.s
    number_pages = args.n
    save_path = args.p

    urls = [(i, '{}?page={}'.format(DOMAIN, i)) for i in range(start_page, start_page + number_pages)]

    for n_page, url in  urls:
        exceptList = []
        tasks = []
        logWarn('Download page: {0} ...'.format(n_page))

        response = requests.get(url=url)
        soup = BeautifulSoup(response.text, "lxml")
        item_links = soup.find(id="search-results").find_all('a', limit=10)

        tasks = []
        for item in item_links:
            href = item['href']
            try:
                download_link = parse_page(href)
            except:
                logErr("Invalid url: {}".format(href))
                exceptList.append(href)
                continue

            task = DownloadThread(target=download, args=(download_link, save_path,))
            task.start()
            tasks.append(task)
            log("... downloading: {}".format(download_link.split('/')[-1].split('?')[0]))

        for task in tasks:
            file_name = task.get_arg(0).split('/')[-1].split('?')[0]
            if task.join() == 0:
                log("success! {}".format(file_name))
            else:
                log("fail!    {}".format(file_name), "e")
                exceptList.append(task.get_arg(0))

        logErr("...total:({0}) invalid".format(len(exceptList)))

def parse_page(link):
    response = requests.get(url=link)
    soup = BeautifulSoup(response.text, "lxml")
    return soup.find(id="product-upper-container").find('div').find('div').find('div')['data-dl-url']

def download(download_link, save_path):    
    file_name = download_link.split('/')[-1].split('?')[0]
    if not file_name:
        return -1

    is_r_sucess = False
    for _ in range(3):
        try:
            r = requests.get(download_link)
            if r.status_code == 200:
                is_r_sucess = True
        except:
            pass

        if is_r_sucess:
          break

    if not is_r_sucess:
      return -1

    with open(os.path.join(save_path, file_name), 'wb+') as f:
        f.write(r.content)
    return 0

class DownloadThread(threading.Thread):
    def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None):
        super(DownloadThread, self).__init__(group, target, name, args, kwargs, daemon=daemon)

        self._return = None

    def run(self):
        if self._target is not None:
            self._return = self._target(*self._args, **self._kwargs)

    def join(self):
        super(DownloadThread, self).join()
        return self._return

    def get_arg(self, pos):
       return self._args[pos] 
