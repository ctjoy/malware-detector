#!/usr/bin/python3
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import sys
import subprocess
import requests
import argparse

from config.proj import DOMAIN, SAVE_PATH
from src.ultis import log, logErr, logWarn

__version__ = '1.2'

def crawler(argv):
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='''
         A crawler for the malware detector, it download benign executable files from Download.com.
         Input: number pages you want to crawl from its software page rank and the save path
         Output: executable files
    ''')
    parser.add_argument('-s', metavar='START_PAGE', help='Number pages you want to START crawl')
    parser.add_argument('-n', metavar='NUMBER_PAGES', help='Number pages you want to crawl')
    parser.add_argument('-p', metavar='SAVE_PATH', help="Save path for executable files")
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)

    args = parser.parse_args(argv[1:])
    start_page = int(args.s) if args.s else 1
    number_pages = int(args.n) if args.n else 1
    save_path = args.p if args.p else SAVE_PATH

    urls = [(i, '{}?page={}'.format(DOMAIN, i)) for i in range(start_page, start_page + number_pages)]

    for n_page, url in  urls:
        exceptList = []
        tasks = []
        logWarn('Download page: {0} ...'.format(n_page))

        response = requests.get(url=url)
        soup = BeautifulSoup(response.text, "lxml")
        item_links = soup.find(id="search-results").find_all('a', limit=10)

        for item in item_links:
            href = item['href']
            try:
                download_link = parse_page(href)
                download(download_link, save_path)
            except:
                print ("Invalid url: {}".format(item['href']))
                exceptList.append(href)

        log("...total:({0}) invalid".format(len(exceptList)))

def parse_page(link):
    response = requests.get(url=link)
    soup = BeautifulSoup(response.text, "lxml")
    return soup.find(id="product-upper-container").find('div').find('div').find('div')['data-dl-url']

def download(download_link, save_path):
    status = subprocess.call(["wget", "-qP", save_path, download_link])
    file_name = download_link.split('/')[-1].split('?')[0]
    if status == 0:
        log(file_name)
    else:
        logErr(file_name)
